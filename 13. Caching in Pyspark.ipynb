{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ef5930",
   "metadata": {},
   "source": [
    "## Caching a DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae9553",
   "metadata": {},
   "source": [
    "Pyspark cache() method is used to cache the intermediate results of the transformation so that other transformation runs on top of cached will perform faster. Caching the result of the transformation is one of the optimization tricks to improve the performance of the long-running PySpark applications/jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e09ae0",
   "metadata": {},
   "source": [
    "cache() is a lazy evaluation in PySpark meaning it will not cache the results until you call the action operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f95565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\sparkhome'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7f3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"caching_dataset\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb4f27",
   "metadata": {},
   "source": [
    "##### Benefits of Caching in PySpark\n",
    "\n",
    "Caching a DataFrame that is reused across multiple operations can greatly enhance the performance of PySpark jobs. Here are some key benefits of using the `cache()` method:\n",
    "\n",
    "- **Cost-Efficient:** Spark computations can be costly, so reusing previously computed results helps in reducing overall costs.\n",
    "- **Time-Efficient:** By avoiding repeated computations, caching saves a significant amount of time.\n",
    "- **Reduced Execution Time:** Caching reduces the execution time of jobs, allowing for more efficient use of the cluster and the ability to run additional jobs.\n",
    "\n",
    "To illustrate the importance of caching, consider a scenario where multiple PySpark transformations are applied in a sequence. Using caching for intermediate results can drastically improve the performance of subsequent transformations that depend on these results.\n",
    "\n",
    "##### Why Use Caching in PySpark?\n",
    "\n",
    "To understand the need for caching, let’s first observe the performance of transformations without caching. This will help in identifying the performance issues and demonstrate the benefits of caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac08c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv(\"employees.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18b70c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|\n",
      "|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|\n",
      "|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|\n",
      "|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|\n",
      "|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|\n",
      "|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|\n",
      "|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|\n",
      "|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|\n",
      "|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|\n",
      "|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|\n",
      "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|\n",
      "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|\n",
      "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|\n",
      "|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|\n",
      "|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|            - |       103|           60|\n",
      "|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|            - |       103|           60|\n",
      "|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|            - |       103|           60|\n",
      "|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|\n",
      "|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|\n",
      "|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ced607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "EMPLOYEE_ID: int, FIRST_NAME: string, LAST_NAME: string, EMAIL: string, PHONE_NUMBER: string, HIRE_DATE: string, JOB_ID: string, SALARY: int, COMMISSION_PCT: string, MANAGER_ID: string, DEPARTMENT_ID: int\n",
      "Relation [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/durga.vamsikumar/Git/Spark/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<EMPLOYEE_ID:int,FIRST_NAME:string,LAST_NAME:string,EMAIL:string,PHONE_NUMBER:string,HIRE_D...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f29e82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark_df.filter(spark_df[\"HIRE_DATE\"]>= \"21-SEP-05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5477eb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f09166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df1.filter(df1[\"JOB_ID\"]==\"AC_ACCOUNT\")\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d75ec",
   "metadata": {},
   "source": [
    "##### What is the issue in the above statement?\n",
    "\n",
    "Let's assume you have billions of records in `employees.csv`. Since actions trigger transformations, in the given example, `df1.count()` is the first action. This action initiates the execution of reading the CSV file and applying `spark_df.filter()`.\n",
    "\n",
    "We also have another action, `df2.count()`, which again triggers the execution of reading the file, `spark_df.where()`, and `df1.where()`.\n",
    "\n",
    "Therefore, in this example, we are reading the file twice and applying `df.filter()` twice. When dealing with a large number of records, this becomes a performance issue. However, it can be easily avoided by caching the results of `spark.read()` and `df2.filter()`. In the following section, I will explain how to use `cache()` to prevent this double execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98d07b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Filter (JOB_ID#23 = AC_ACCOUNT)\n",
      "+- Filter (HIRE_DATE#22 >= 21-SEP-05)\n",
      "   +- Relation [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "EMPLOYEE_ID: int, FIRST_NAME: string, LAST_NAME: string, EMAIL: string, PHONE_NUMBER: string, HIRE_DATE: string, JOB_ID: string, SALARY: int, COMMISSION_PCT: string, MANAGER_ID: string, DEPARTMENT_ID: int\n",
      "Filter (JOB_ID#23 = AC_ACCOUNT)\n",
      "+- Filter (HIRE_DATE#22 >= 21-SEP-05)\n",
      "   +- Relation [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter ((isnotnull(HIRE_DATE#22) AND isnotnull(JOB_ID#23)) AND ((HIRE_DATE#22 >= 21-SEP-05) AND (JOB_ID#23 = AC_ACCOUNT)))\n",
      "+- Relation [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((isnotnull(HIRE_DATE#22) AND isnotnull(JOB_ID#23)) AND (HIRE_DATE#22 >= 21-SEP-05)) AND (JOB_ID#23 = AC_ACCOUNT))\n",
      "+- FileScan csv [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] Batched: false, DataFilters: [isnotnull(HIRE_DATE#22), isnotnull(JOB_ID#23), (HIRE_DATE#22 >= 21-SEP-05), (JOB_ID#23 = AC_ACCO..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/durga.vamsikumar/Git/Spark/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(HIRE_DATE), IsNotNull(JOB_ID), GreaterThanOrEqual(HIRE_DATE,21-SEP-05), EqualTo(JOB_ID..., ReadSchema: struct<EMPLOYEE_ID:int,FIRST_NAME:string,LAST_NAME:string,EMAIL:string,PHONE_NUMBER:string,HIRE_D...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13001da8",
   "metadata": {},
   "source": [
    "### Below is the syntax of cache() on DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab8e74",
   "metadata": {},
   "source": [
    "###### Syntax\n",
    "\n",
    "DataFrame.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff63db0",
   "metadata": {},
   "source": [
    "let’s add cache() statement to spark.read() and spark_df.filter() transformations. When df2.count() executes, this triggers spark.read.csv(..).cache() which reads the file and caches the result in memory. and df.filter(..).cache() also caches the result in memory.\n",
    "\n",
    "When cache_df.count() executes, it just performs the df2.where() on top of cache results of df2, without re-executing previous transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9f1a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv(\"employees.csv\",header=True,inferSchema=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff4b43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_df = spark_df.filter(spark_df[\"HIRE_DATE\"] >= \"21-SEP-05\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e0d8fa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71e9d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Filter (HIRE_DATE#528 >= 21-SEP-05)\n",
      "+- Relation [EMPLOYEE_ID#523,FIRST_NAME#524,LAST_NAME#525,EMAIL#526,PHONE_NUMBER#527,HIRE_DATE#528,JOB_ID#529,SALARY#530,COMMISSION_PCT#531,MANAGER_ID#532,DEPARTMENT_ID#533] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "EMPLOYEE_ID: int, FIRST_NAME: string, LAST_NAME: string, EMAIL: string, PHONE_NUMBER: string, HIRE_DATE: string, JOB_ID: string, SALARY: int, COMMISSION_PCT: string, MANAGER_ID: string, DEPARTMENT_ID: int\n",
      "Filter (HIRE_DATE#528 >= 21-SEP-05)\n",
      "+- Relation [EMPLOYEE_ID#523,FIRST_NAME#524,LAST_NAME#525,EMAIL#526,PHONE_NUMBER#527,HIRE_DATE#528,JOB_ID#529,SALARY#530,COMMISSION_PCT#531,MANAGER_ID#532,DEPARTMENT_ID#533] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InMemoryRelation [EMPLOYEE_ID#523, FIRST_NAME#524, LAST_NAME#525, EMAIL#526, PHONE_NUMBER#527, HIRE_DATE#528, JOB_ID#529, SALARY#530, COMMISSION_PCT#531, MANAGER_ID#532, DEPARTMENT_ID#533], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   +- *(1) Filter (isnotnull(HIRE_DATE#22) AND (HIRE_DATE#22 >= 21-SEP-05))\n",
      "      +- FileScan csv [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] Batched: false, DataFilters: [isnotnull(HIRE_DATE#22), (HIRE_DATE#22 >= 21-SEP-05)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/durga.vamsikumar/Git/Spark/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(HIRE_DATE), GreaterThanOrEqual(HIRE_DATE,21-SEP-05)], ReadSchema: struct<EMPLOYEE_ID:int,FIRST_NAME:string,LAST_NAME:string,EMAIL:string,PHONE_NUMBER:string,HIRE_D...\n",
      "\n",
      "== Physical Plan ==\n",
      "InMemoryTableScan [EMPLOYEE_ID#523, FIRST_NAME#524, LAST_NAME#525, EMAIL#526, PHONE_NUMBER#527, HIRE_DATE#528, JOB_ID#529, SALARY#530, COMMISSION_PCT#531, MANAGER_ID#532, DEPARTMENT_ID#533]\n",
      "   +- InMemoryRelation [EMPLOYEE_ID#523, FIRST_NAME#524, LAST_NAME#525, EMAIL#526, PHONE_NUMBER#527, HIRE_DATE#528, JOB_ID#529, SALARY#530, COMMISSION_PCT#531, MANAGER_ID#532, DEPARTMENT_ID#533], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Filter (isnotnull(HIRE_DATE#22) AND (HIRE_DATE#22 >= 21-SEP-05))\n",
      "            +- FileScan csv [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] Batched: false, DataFilters: [isnotnull(HIRE_DATE#22), (HIRE_DATE#22 >= 21-SEP-05)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/durga.vamsikumar/Git/Spark/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(HIRE_DATE), GreaterThanOrEqual(HIRE_DATE,21-SEP-05)], ReadSchema: struct<EMPLOYEE_ID:int,FIRST_NAME:string,LAST_NAME:string,EMAIL:string,PHONE_NUMBER:string,HIRE_D...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cache_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee981594",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_department_id_df = cache_df.groupBy(\"DEPARTMENT_ID\").agg({'SALARY':'SUM'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a27b1ec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_department_id_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "803c3ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['DEPARTMENT_ID], ['DEPARTMENT_ID, 'sum(SALARY#530) AS sum(SALARY)#903]\n",
      "+- Filter (HIRE_DATE#528 >= 21-SEP-05)\n",
      "   +- Relation [EMPLOYEE_ID#523,FIRST_NAME#524,LAST_NAME#525,EMAIL#526,PHONE_NUMBER#527,HIRE_DATE#528,JOB_ID#529,SALARY#530,COMMISSION_PCT#531,MANAGER_ID#532,DEPARTMENT_ID#533] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DEPARTMENT_ID: int, sum(SALARY): bigint\n",
      "Aggregate [DEPARTMENT_ID#533], [DEPARTMENT_ID#533, sum(SALARY#530) AS sum(SALARY)#903L]\n",
      "+- Filter (HIRE_DATE#528 >= 21-SEP-05)\n",
      "   +- Relation [EMPLOYEE_ID#523,FIRST_NAME#524,LAST_NAME#525,EMAIL#526,PHONE_NUMBER#527,HIRE_DATE#528,JOB_ID#529,SALARY#530,COMMISSION_PCT#531,MANAGER_ID#532,DEPARTMENT_ID#533] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [DEPARTMENT_ID#533], [DEPARTMENT_ID#533, sum(SALARY#530) AS sum(SALARY)#903L]\n",
      "+- Project [SALARY#530, DEPARTMENT_ID#533]\n",
      "   +- InMemoryRelation [EMPLOYEE_ID#523, FIRST_NAME#524, LAST_NAME#525, EMAIL#526, PHONE_NUMBER#527, HIRE_DATE#528, JOB_ID#529, SALARY#530, COMMISSION_PCT#531, MANAGER_ID#532, DEPARTMENT_ID#533], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Filter (isnotnull(HIRE_DATE#22) AND (HIRE_DATE#22 >= 21-SEP-05))\n",
      "            +- FileScan csv [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] Batched: false, DataFilters: [isnotnull(HIRE_DATE#22), (HIRE_DATE#22 >= 21-SEP-05)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/durga.vamsikumar/Git/Spark/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(HIRE_DATE), GreaterThanOrEqual(HIRE_DATE,21-SEP-05)], ReadSchema: struct<EMPLOYEE_ID:int,FIRST_NAME:string,LAST_NAME:string,EMAIL:string,PHONE_NUMBER:string,HIRE_D...\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEPARTMENT_ID#533], functions=[sum(SALARY#530)], output=[DEPARTMENT_ID#533, sum(SALARY)#903L])\n",
      "   +- Exchange hashpartitioning(DEPARTMENT_ID#533, 200), ENSURE_REQUIREMENTS, [plan_id=368]\n",
      "      +- HashAggregate(keys=[DEPARTMENT_ID#533], functions=[partial_sum(SALARY#530)], output=[DEPARTMENT_ID#533, sum#1355L])\n",
      "         +- InMemoryTableScan [SALARY#530, DEPARTMENT_ID#533]\n",
      "               +- InMemoryRelation [EMPLOYEE_ID#523, FIRST_NAME#524, LAST_NAME#525, EMAIL#526, PHONE_NUMBER#527, HIRE_DATE#528, JOB_ID#529, SALARY#530, COMMISSION_PCT#531, MANAGER_ID#532, DEPARTMENT_ID#533], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) Filter (isnotnull(HIRE_DATE#22) AND (HIRE_DATE#22 >= 21-SEP-05))\n",
      "                        +- FileScan csv [EMPLOYEE_ID#17,FIRST_NAME#18,LAST_NAME#19,EMAIL#20,PHONE_NUMBER#21,HIRE_DATE#22,JOB_ID#23,SALARY#24,COMMISSION_PCT#25,MANAGER_ID#26,DEPARTMENT_ID#27] Batched: false, DataFilters: [isnotnull(HIRE_DATE#22), (HIRE_DATE#22 >= 21-SEP-05)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/durga.vamsikumar/Git/Spark/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(HIRE_DATE), GreaterThanOrEqual(HIRE_DATE,21-SEP-05)], ReadSchema: struct<EMPLOYEE_ID:int,FIRST_NAME:string,LAST_NAME:string,EMAIL:string,PHONE_NUMBER:string,HIRE_D...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_department_id_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31bbdfa",
   "metadata": {},
   "source": [
    "### Summary of Each Stage\n",
    "\n",
    "1. **Parsed Logical Plan**:\n",
    "   - Represents the initial query parsing, showing a filter operation on the data read from a CSV file.\n",
    "\n",
    "2. **Analyzed Logical Plan**:\n",
    "   - Displays the schema of the data and confirms the filter operation.\n",
    "\n",
    "3. **Optimized Logical Plan**:\n",
    "   - Indicates that the data is cached in memory, optimizing the filter operation to avoid re-reading from disk.\n",
    "\n",
    "4. **Physical Plan**:\n",
    "   - Confirms that subsequent actions will utilize the cached data in memory, ensuring efficient execution without redundant reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c104477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform subsequent transformations or analyses on cached dataset\n",
    "# For example: calculating average transaction value\n",
    "average_transaction_value_df = cache_df \\\n",
    "    .groupBy(\"job_id\",\"department_id\") \\\n",
    "    .agg({\"salary\": \"avg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66d70147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+\n",
      "|    job_id|department_id|avg(salary)|\n",
      "+----------+-------------+-----------+\n",
      "|FI_ACCOUNT|          100|     7950.0|\n",
      "|  ST_CLERK|           50|     2900.0|\n",
      "|     AD_VP|           90|    17000.0|\n",
      "|   IT_PROG|           60|     4800.0|\n",
      "|  PU_CLERK|           30|     2850.0|\n",
      "+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_transaction_value_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b20e7b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[EMPLOYEE_ID: int, FIRST_NAME: string, LAST_NAME: string, EMAIL: string, PHONE_NUMBER: string, HIRE_DATE: string, JOB_ID: string, SALARY: int, COMMISSION_PCT: string, MANAGER_ID: string, DEPARTMENT_ID: int]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unpersist the cached dataset after use\n",
    "cache_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a25bc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e1c7e",
   "metadata": {},
   "source": [
    "PySpark cache() method is used to cache the intermediate results of the transformation into memory so that any future transformations on the results of cached transformation improve the performance. Caching is a lazy evaluation meaning it will not cache the results until you call the action operation and the result of the transformation is one of the optimization tricks to improve the performance of the long-running PySpark applications/jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161259fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
