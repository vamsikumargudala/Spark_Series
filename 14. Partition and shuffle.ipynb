{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436027aa",
   "metadata": {},
   "source": [
    "### How Shuffle Works Without Multiple Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab0a64",
   "metadata": {},
   "source": [
    "Even if you are running Spark on a single node without multiple executors, the shuffle process can still occur. The shuffle is an intrinsic part of certain operations like `reduceByKey`, `groupByKey`, `join`, etc., and it refers to the process of redistributing data across the partitions.\n",
    "\n",
    "1. **Single Executor**: When running Spark on a single node, there is only one executor. This executor manages all the tasks, so the data does not move across different machines, but it still gets reorganized within the node.\n",
    "\n",
    "2. **Shuffling within a Single Node**: The data is divided into partitions, and the shuffle operation involves redistributing the data among these partitions. Even on a single node, Spark will still perform the shuffle to ensure that the data is correctly grouped or sorted according to the requirements of the transformation.\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "1. **Partitions**: \n",
    "   - The input data (e.g., from a file) is split into multiple partitions. In Spark, each partition is a logical chunk of data that can be processed independently.\n",
    "\n",
    "2. **Map Phase**:\n",
    "   - During the map phase of a shuffle operation, each task processes a partition and generates intermediate key-value pairs. For example, in `reduceByKey`, it maps each element to a key-value pair.\n",
    "\n",
    "3. **Shuffle Write**:\n",
    "   - The intermediate data (key-value pairs) is written to disk (or memory) as shuffle files. Each partition will generate a file for every reducer.\n",
    "\n",
    "4. **Shuffle Read**:\n",
    "   - In the reduce phase, the data is read from these shuffle files. Each task reads the relevant data for its partition from the shuffle files.\n",
    "\n",
    "5. **Reduce Phase**:\n",
    "   - The reduce tasks combine the data based on the key. For `reduceByKey`, this means summing up the values for each key.\n",
    "\n",
    "Even without multiple executors, these steps ensure that data is correctly partitioned and grouped. The main difference is that the data is shuffled within the same physical machine rather than across different machines."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8006d145",
   "metadata": {},
   "source": [
    "Hereâ€™s an example of how the shuffle operation works even on a single node:\n",
    "\n",
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Shuffle Example\").getOrCreate()\n",
    "\n",
    "# Get the SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Create an RDD from a text file\n",
    "lines_rdd = sc.textFile(\"example.txt\")\n",
    "\n",
    "# Split each line into words\n",
    "words_rdd = lines_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Map each word to a tuple (word, 1)\n",
    "pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key (sum the counts for each word)\n",
    "wordCounts_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect and print the results\n",
    "for word, count in wordCounts_rdd.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4050ad",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "1. **Reading the File**: The `textFile` method reads the `example.txt` file and splits it into partitions.\n",
    "2. **FlatMap**: The `flatMap` transformation splits each line into words.\n",
    "3. **Map**: The `map` transformation creates a key-value pair for each word.\n",
    "4. **ReduceByKey**: The `reduceByKey` transformation triggers a shuffle, redistributing the key-value pairs so that all pairs with the same key end up in the same partition. This is where the shuffle happens.\n",
    "5. **Collect**: The `collect` action gathers the results from all partitions.\n",
    "\n",
    "### Shuffle within Single Node\n",
    "\n",
    "- **Intermediate Data**: During the `reduceByKey` operation, intermediate data is shuffled within the single node. The shuffle involves writing the intermediate results to disk (or memory) and reading them again to combine the results.\n",
    "- **Efficiency**: Even though no network transfer is needed, the shuffle within a single node still incurs a performance cost due to disk I/O and data serialization/deserialization.\n",
    "\n",
    "In summary, the shuffle operation in Spark is a crucial part of data processing that ensures the data is correctly partitioned and aggregated. This process happens regardless of whether you are using multiple executors or a single executor on a single node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e2f89",
   "metadata": {},
   "source": [
    "To check the number of partitions of an RDD in Spark, you can use the `getNumPartitions` method. This method returns the number of partitions in the RDD. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa710d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Number of partitions after flatMap: 2\n",
      "Number of partitions after map: 2\n",
      "Number of partitions after reduceByKey: 2\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Check Partitions Example\").getOrCreate()\n",
    "\n",
    "# Get the SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Create an RDD from a text file\n",
    "lines_rdd = sc.textFile(\"input.txt\")\n",
    "\n",
    "# Check the number of partitions\n",
    "num_partitions = lines_rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "# Split each line into words\n",
    "words_rdd = lines_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Check the number of partitions after transformation\n",
    "num_partitions_words = words_rdd.getNumPartitions()\n",
    "print(f\"Number of partitions after flatMap: {num_partitions_words}\")\n",
    "\n",
    "# Map each word to a tuple (word, 1)\n",
    "pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# Check the number of partitions after transformation\n",
    "num_partitions_pairs = pairs_rdd.getNumPartitions()\n",
    "print(f\"Number of partitions after map: {num_partitions_pairs}\")\n",
    "\n",
    "# Reduce by key (sum the counts for each word)\n",
    "wordCounts_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Check the number of partitions after reduceByKey\n",
    "num_partitions_wordCounts = wordCounts_rdd.getNumPartitions()\n",
    "print(f\"Number of partitions after reduceByKey: {num_partitions_wordCounts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164d7db",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "1. **Create an RDD**: Read a text file and create an RDD.\n",
    "2. **Check Partitions**: Use the `getNumPartitions` method to check the number of partitions at different stages of the RDD transformations.\n",
    "3. **Transformations**:\n",
    "   - `flatMap`: Splits each line into words.\n",
    "   - `map`: Maps each word to a tuple (word, 1).\n",
    "   - `reduceByKey`: Reduces by key to count the occurrences of each word.\n",
    "4. **Print Partitions**: Print the number of partitions after each transformation to see how they change.\n",
    "\n",
    "### Output\n",
    "\n",
    "This code will print the number of partitions at different stages of the RDD transformations. You will see how the number of partitions changes (if at all) after each transformation.\n",
    "\n",
    "### Additional Information\n",
    "\n",
    "- **Setting the Number of Partitions**: You can also set the number of partitions when reading the file or during transformations. For example, you can specify the number of partitions when reading a text file:\n",
    "\n",
    "```python\n",
    "lines_rdd = sc.textFile(\"example.txt\", minPartitions=4)\n",
    "```\n",
    "\n",
    "- **Repartitioning**: If you want to change the number of partitions of an existing RDD, you can use the `repartition` or `coalesce` methods. `repartition` allows you to increase or decrease the number of partitions, while `coalesce` is optimized for decreasing the number of partitions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce3a4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions after repartition: 4\n",
      "Number of partitions after coalesce: 2\n"
     ]
    }
   ],
   "source": [
    "# Repartition the RDD to 4 partitions\n",
    "repartitioned_rdd = lines_rdd.repartition(4)\n",
    "print(f\"Number of partitions after repartition: {repartitioned_rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce the RDD to 2 partitions\n",
    "coalesced_rdd = lines_rdd.coalesce(2)\n",
    "print(f\"Number of partitions after coalesce: {coalesced_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30025498",
   "metadata": {},
   "source": [
    "By checking and managing the number of partitions, you can optimize the performance of your Spark jobs, balancing the workload across partitions and minimizing data shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab6034",
   "metadata": {},
   "source": [
    "### How Shuffle Works With Multiple Executors by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd57cfe",
   "metadata": {},
   "source": [
    "In a Spark cluster, the default number of partitions for an RDD is determined by the configuration of the cluster and the type of input data source. To check and understand the default partitioning behavior, you can follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565cb69a",
   "metadata": {},
   "source": [
    "In a Spark cluster, the default number of partitions for an RDD is determined by the configuration of the cluster and the type of input data source. To check and understand the default partitioning behavior, you can follow these steps:\n",
    "\n",
    "1. **Check the Configuration**:\n",
    "   - The default number of partitions for RDDs can be influenced by Spark configuration parameters. The most relevant parameters are:\n",
    "     - `spark.default.parallelism`: This configuration controls the default number of partitions for RDDs that are created from in-memory collections (e.g., `parallelize`).\n",
    "     - `spark.sql.shuffle.partitions`: This configuration controls the default number of partitions for DataFrame and Dataset shuffles.\n",
    "   - You can check these configurations in your Spark application as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13362a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Check Default Partitions\").getOrCreate()\n",
    "\n",
    "# Get SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Check the default parallelism\n",
    "default_parallelism = sc.defaultParallelism\n",
    "print(f\"Default parallelism: {default_parallelism}\")\n",
    "\n",
    "# Check the default shuffle partitions\n",
    "default_shuffle_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"Default shuffle partitions: {default_shuffle_partitions}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05193332",
   "metadata": {},
   "source": [
    "\n",
    "2. **Reading a File**:\n",
    "- When reading from HDFS, S3, or other distributed file systems, the default number of partitions is usually determined by the block size of the file system and the size of the input files. You can check the number of partitions of an RDD created from a file as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25261342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Check File Partitions\").getOrCreate()\n",
    "\n",
    "# Get SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Read a file into an RDD\n",
    "lines_rdd = sc.textFile(\"hdfs:///path/to/your/file\")\n",
    "\n",
    "# Check the number of partitions\n",
    "num_partitions = lines_rdd.getNumPartitions()\n",
    "print(f\"Number of partitions for the file RDD: {num_partitions}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72aed3b",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "1. **Checking Configuration**:\n",
    "   - `sc.defaultParallelism`: This property gives the default level of parallelism (number of partitions) used when creating RDDs from in-memory collections (e.g., using `sc.parallelize`). It is generally set to the number of cores available to your Spark application.\n",
    "   - `spark.sql.shuffle.partitions`: This property determines the number of partitions to use when shuffling data for joins or aggregations in DataFrame operations. By default, this is set to 200.\n",
    "\n",
    "2. **Reading a File**:\n",
    "   - The number of partitions when reading from a distributed file system depends on the file block size and the total size of the files. Spark will create one partition for each block of the file, allowing it to process data in parallel.\n",
    "\n",
    "### Additional Information\n",
    "\n",
    "- **Customizing Partitions**: If the default partitioning is not suitable for your application, you can customize the number of partitions when reading data or repartitioning existing RDDs/DataFrames:\n",
    "\n",
    "  ```python\n",
    "  # Reading a file with a custom number of partitions\n",
    "  lines_rdd = sc.textFile(\"hdfs:///path/to/your/file\", minPartitions=8)\n",
    "  print(f\"Number of partitions after setting minPartitions: {lines_rdd.getNumPartitions()}\")\n",
    "\n",
    "  # Repartitioning an existing RDD\n",
    "  repartitioned_rdd = lines_rdd.repartition(4)\n",
    "  print(f\"Number of partitions after repartition: {repartitioned_rdd.getNumPartitions()}\")\n",
    "  ```\n",
    "\n",
    "- **Optimizing Performance**: Properly setting the number of partitions can improve the performance of your Spark jobs by ensuring that tasks are evenly distributed and that shuffling is minimized. It is important to balance the workload across partitions and avoid creating too few or too many partitions.\n",
    "\n",
    "By checking and understanding the default partitioning in your Spark cluster, you can better optimize your data processing workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20ba99",
   "metadata": {},
   "source": [
    "When working with Spark SQL, you can control the number of partitions used during various operations such as reading data, shuffling, and joins by setting appropriate configurations. Here are some common scenarios and how to set the number of partitions for them:\n",
    "\n",
    "### 1. **Setting the Number of Partitions for Reading Data**\n",
    "\n",
    "When reading data using Spark SQL, you can specify the number of partitions directly in the `DataFrameReader` API for certain data sources.\n",
    "\n",
    "#### Example: Reading from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Set Partitions for Reading Data\").getOrCreate()\n",
    "\n",
    "# Read a CSV file with a custom number of partitions\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"hdfs:///path/to/your/file.csv\")\n",
    "\n",
    "# Repartition the DataFrame to the desired number of partitions\n",
    "df = df.repartition(8)\n",
    "print(f\"Number of partitions after repartition: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f40c49",
   "metadata": {},
   "source": [
    "### 2. **Setting the Number of Shuffle Partitions**\n",
    "\n",
    "The number of partitions used during shuffle operations (such as joins, groupBy, etc.) in Spark SQL can be controlled using the `spark.sql.shuffle.partitions` configuration parameter.\n",
    "\n",
    "#### Example: Setting the shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession with custom shuffle partitions\n",
    "spark = SparkSession.builder.appName(\"Set Shuffle Partitions\").config(\"spark.sql.shuffle.partitions\", \"8\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29), (\"David\", 31)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Perform a groupBy operation to trigger a shuffle\n",
    "grouped_df = df.groupBy(\"Name\").count()\n",
    "\n",
    "# Show the DataFrame\n",
    "grouped_df.show()\n",
    "\n",
    "# Check the number of partitions after shuffle\n",
    "print(f\"Number of shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9958d7",
   "metadata": {},
   "source": [
    "### 3. **Repartitioning DataFrames**\n",
    "\n",
    "You can also use the `repartition` and `coalesce` methods on a DataFrame to explicitly set the number of partitions.\n",
    "\n",
    "#### Example: Repartitioning a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c4d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Repartition DataFrame\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29), (\"David\", 31)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Repartition the DataFrame to 4 partitions\n",
    "repartitioned_df = df.repartition(4)\n",
    "print(f\"Number of partitions after repartition: {repartitioned_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce the DataFrame to 2 partitions\n",
    "coalesced_df = df.coalesce(2)\n",
    "print(f\"Number of partitions after coalesce: {coalesced_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49d99b",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- **`spark.sql.shuffle.partitions`**: Use this configuration to control the number of partitions used during shuffle operations. You can set this when initializing the `SparkSession`.\n",
    "- **`repartition` and `coalesce`**: Use these methods to explicitly control the number of partitions for a DataFrame.\n",
    "- **Directly in DataFrameReader API**: For some data sources, you can specify the number of partitions directly when reading the data.\n",
    "\n",
    "By carefully setting and managing the number of partitions, you can optimize the performance of your Spark SQL queries and ensure efficient use of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c2f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
