{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fd9c7a",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe1391",
   "metadata": {},
   "source": [
    "The terms **\"normalized\" and \"denormalized\"** have been part of database terminology for a long time. Although they might seem confusing at first, these concepts are relatively simple to understand, implement, and leverage.\n",
    "\n",
    "**Please note** that this is not an in-depth lesson on Data Modeling or Data Warehousing. It will provide a simplified explanation of \"normalized\" and \"denormalized\" for the sake of clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7754e",
   "metadata": {},
   "source": [
    "+ **Normalized Data:** In normalized data, information about a subject is distributed across multiple tables or datasets. This approach organizes data into several tables, with each table containing specific details about the subject. To retrieve a comprehensive view of the data, you need to join these tables using an identifying data point, such as an ID, name, serial number, or other unique identifier (which could even be a physical address, business name, email address, etc.).\n",
    "\n",
    "In summary, working with normalized data often requires querying multiple tables and using reference datasets to fill in missing information. It's a more complex structure but can be efficient in certain scenarios.\n",
    "\n",
    "**TL;DR:** Normalized data involves multiple tables, and you need to join them to get a complete view of the data.\n",
    "\n",
    "+ **Denormalized Data:** In contrast, denormalized data is stored in a single table, providing a convenient and comprehensive view of the information. All relevant data is present in this one table, making queries more straightforward and potentially faster.\n",
    "\n",
    "So, when should you use normalized vs. denormalized datasets? The decision depends on various factors, including storage space, performance, the number and cardinality of columns, and usability. It's a topic that deserves a more in-depth discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7c225",
   "metadata": {},
   "source": [
    "In this post, we'll focus on using Spark to denormalize normalized data, a common task in Data Engineering.\n",
    "\n",
    "In previous sections, we used a fictional animal dataset to illustrate Spark examples. Now, let's see what denormalized data can look like, still using our animal dataset as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a9df9",
   "metadata": {},
   "source": [
    "## Denormalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ce6e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name  animal  age  color\n",
      "0       fido     dog    4  brown\n",
      "1  annabelle     cat   15  white\n",
      "2       fred    bear   29  brown\n",
      "3      julie  parrot    1  brown\n",
      "4        gus    fish    1   gold\n",
      "5      daisy  iguana    2  green\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define the list of dictionaries representing the denormalized dataset\n",
    "denormalized_dataset = [\n",
    "    {\"name\": \"fido\", \"animal\": \"dog\", \"age\": 4, \"color\": \"brown\"},\n",
    "    {\"name\": \"annabelle\", \"animal\": \"cat\", \"age\": 15, \"color\": \"white\"},\n",
    "    {\"name\": \"fred\", \"animal\": \"bear\", \"age\": 29, \"color\": \"brown\"},\n",
    "    {\"name\": \"julie\", \"animal\": \"parrot\", \"age\": 1, \"color\": \"brown\"},\n",
    "    {\"name\": \"gus\", \"animal\": \"fish\", \"age\": 1, \"color\": \"gold\"},\n",
    "    {\"name\": \"daisy\", \"animal\": \"iguana\", \"age\": 2, \"color\": \"green\"}\n",
    "]\n",
    "\n",
    "# Print the denormalized dataset\n",
    "# Create a DataFrame from the denormalized dataset\n",
    "df = pd.DataFrame(denormalized_dataset)\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca582d",
   "metadata": {},
   "source": [
    "An example of our animal data in a normalized format might look like the following, where the fields for animal and color are represented by numeric codes instead of their actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec024e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name  animal  age  color\n",
      "0       fido       1    4      1\n",
      "1  annabelle       2   15      2\n",
      "2       fred       3   29      1\n",
      "3      julie       4    1      1\n",
      "4        gus       5    1      4\n",
      "5      daisy       6    2      5\n"
     ]
    }
   ],
   "source": [
    "# Define the data as a dictionary\n",
    "data = {\n",
    "    \"name\": [\"fido\", \"annabelle\", \"fred\", \"julie\", \"gus\", \"daisy\"],\n",
    "    \"animal\": [1, 2, 3, 4, 5, 6],\n",
    "    \"age\": [4, 15, 29, 1, 1, 2],\n",
    "    \"color\": [1, 2, 1, 1, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69125f",
   "metadata": {},
   "source": [
    "There are situations where you may need to normalize denormalized data or denormalize normalized data, and Spark is a powerful tool for handling these transformations. This capability is not unique to Spark but demonstrates a common and essential data manipulation task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b118b85",
   "metadata": {},
   "source": [
    "Here's a high-level overview of the trade-offs between normalized and denormalized data:\n",
    "\n",
    "### Normalized Data:\n",
    "\n",
    "+ Reduces data duplication.\n",
    "+ Enhances data integrity by maintaining data consistency.\n",
    "+ Requires joining multiple tables, potentially making queries slower due to the overhead of these joins.\n",
    "\n",
    "### Denormalized Data:\n",
    "\n",
    "+ Can offer faster query performance because it contains all data points in a single table as literal values.\n",
    "+ May lead to duplicate records.\n",
    "+ Could result in reduced data integrity as data consistency may be harder to maintain.\n",
    "+ Choosing the right data format, like columnar storage, can enhance access efficiency when dealing with wide datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae67ce",
   "metadata": {},
   "source": [
    "Both approaches have their time and place, depending on specific use cases and requirements.\n",
    "\n",
    "In the context of Spark, let's focus on how to convert data from a normalized format to a denormalized format. This transformation can be valuable when you prioritize query speed over data integrity and duplication concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05217704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Vamsi_App_8').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ce2b9",
   "metadata": {},
   "source": [
    "#### Detailed set of instructions for converting normalized data into a denormalized format using Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e3609",
   "metadata": {},
   "source": [
    "**Step 1:** Create Lists of Rows representing normalized data for animals, animal types, and animal colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7daf0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "animalsNormalized = [(\"fido\", 1, 4, 1),\n",
    " (\"annabelle\", 2, 15, 2),\n",
    " (\"fred\", 3, 29, 1),\n",
    " (\"fred\", 4, 1, 1),\n",
    " (\"gus\", 5, 1, 4),\n",
    " (\"daisy\", 6, 2, 5)]\n",
    "\n",
    "animalTypeLookup = [(\"dog\", 1),\n",
    " (\"cat\", 2),\n",
    " (\"bear\", 3),\n",
    " (\"parrot\", 4),\n",
    " (\"fish\", 5),\n",
    " (\"iguana\", 6)]\n",
    "\n",
    "animalColorLookup = [(\"brown\", 1),\n",
    " (\"white\", 2),\n",
    " (\"black\", 3),\n",
    " (\"gold\", 4),\n",
    " (\"green\", 5),\n",
    " (\"red\", 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef8ca1",
   "metadata": {},
   "source": [
    "**Step 2:** Create RDDs from the Lists using sc.parallelize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd0faf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "petsRDD = spark.sparkContext.parallelize(animalsNormalized)\n",
    "colorsRDD = spark.sparkContext.parallelize(animalColorLookup)\n",
    "typesRDD = spark.sparkContext.parallelize(animalTypeLookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51247f18",
   "metadata": {},
   "source": [
    "**Step 3:** Create DataFrames from the RDDs with specified schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e75ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "petsDF = spark.createDataFrame(petsRDD, ['nickname', 'type', 'age', 'color'])\n",
    "colors = spark.createDataFrame(colorsRDD, ['color_name', 'color_id'])\n",
    "types = spark.createDataFrame(typesRDD, ['type_name', 'type_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d8430",
   "metadata": {},
   "source": [
    "**Step 4:** Join the first DataFrame (petsDF) with the second (colors) using the color_id as the join key. Call this new DataFrame petsWithColors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3aa3537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---+\n",
      "| nickname|color_name|age|\n",
      "+---------+----------+---+\n",
      "|     fido|     brown|  4|\n",
      "|annabelle|     white| 15|\n",
      "|     fred|     brown| 29|\n",
      "|     fred|     brown|  1|\n",
      "|      gus|      gold|  1|\n",
      "|    daisy|     green|  2|\n",
      "+---------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "petsWithColors = petsDF.join(colors, col(\"color\") == col(\"color_id\"), how=\"left\")\n",
    "petsWithColors.select(\"nickname\", \"color_name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a48e46",
   "metadata": {},
   "source": [
    "**Step 5:** Join the petsWithColors DataFrame with the types DataFrame using the type_id as the join key. Call this new DataFrame petsWithColorAndType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3573eb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---+----------+\n",
      "| nickname|type_name|age|color_name|\n",
      "+---------+---------+---+----------+\n",
      "|    daisy|   iguana|  2|     green|\n",
      "|      gus|     fish|  1|      gold|\n",
      "|     fido|      dog|  4|     brown|\n",
      "|     fred|     bear| 29|     brown|\n",
      "|annabelle|      cat| 15|     white|\n",
      "|     fred|   parrot|  1|     brown|\n",
      "+---------+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "petsWithColorAndType = petsWithColors.join(types, col(\"type\") == col(\"type_id\"), how=\"left\")\n",
    "petsWithColorAndType.select(\"nickname\", \"type_name\", \"age\", \"color_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa6ddb",
   "metadata": {},
   "source": [
    "This sequence of operations transforms the normalized data into a denormalized format, providing a comprehensive view of the pets' information with their type, age, and color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d830e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
