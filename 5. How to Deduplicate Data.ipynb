{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b433d2c8",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "**Deduplication** is the process of getting rid of duplicate records from a bunch of data. This is usually done early on when dealing with data, before diving into deeper analysis or handing it over to experts like analysts and data scientists. Duplicates can be tricky and can unexpectedly pop up in your final data, so it's important to tackle them.\n",
    "\n",
    "Think of deduplication as a way to tidy up your data. You do this to ensure that your data is more useful and accurate. Sometimes, it's just about tossing out repeated entries or getting rid of stuff you don't need. Luckily, in Spark (a powerful data processing tool), there's a smart and easy function you can use for this purpose called \"dropDuplicates()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda07f7c",
   "metadata": {},
   "source": [
    "![\"Deduplicate Data Image\"](duplicatedata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae4a8a",
   "metadata": {},
   "source": [
    "Let's see how it's done"
   ]
  },
  {
   "cell_type": "raw",
   "id": "195787bd",
   "metadata": {},
   "source": [
    "# Assuming you have a DataFrame named dataFrame\n",
    "deduplicatedDataFrame = dataFrame.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466446c",
   "metadata": {},
   "source": [
    "Cleaning up data is super important to make it more valuable. Sometimes, this just means throwing away repeated stuff or filtering out things you don't want. The good news is that Spark offers a bunch of useful tools that make this whole process really easy. Let's show how Spark's built-in deduplication works by using it on a simple dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d0153",
   "metadata": {},
   "source": [
    "Create a List of tuples containing an animal and its category using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325ae52",
   "metadata": {},
   "source": [
    "List that has some animals and their categories, like \"dog\" being a \"pet,\" \"cat\" being a \"pet,\" and \"bear\" being \"wild.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06790a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_animals = [(\"dog\", \"pet\"), (\"cat\", \"pet\"), (\"bear\", \"wild\"), (\"cat\", \"pet\"), (\"cat\", \"pet\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "984887b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\sparkhome'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ddfa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba354a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Vamsi_App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8795c",
   "metadata": {},
   "source": [
    "### Creating RDD\n",
    "\n",
    "Now, you want to work with this animal data using Spark. You transform this list into something called an RDD (Resilient Distributed Dataset) using Spark's **parallelize()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dcf306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "animalDataRDD = spark.sparkContext.parallelize(categorized_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebecf470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animalDataRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7b9ab",
   "metadata": {},
   "source": [
    "Create a DataFrame from the RDD and print the results to the console using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39789d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "animalsDF = spark.createDataFrame(animalDataRDD,['name','category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5344aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|name|category|\n",
      "+----+--------+\n",
      "| dog|     pet|\n",
      "| cat|     pet|\n",
      "|bear|    wild|\n",
      "| cat|     pet|\n",
      "| cat|     pet|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animalsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5ec83",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0a2fe",
   "metadata": {},
   "source": [
    "Since there are duplicate rows (like \"cat\" appearing multiple times with the \"pet\" category), you want to clean it up. You do this using the dropDuplicates() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53b2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated = animalsDF.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b9d7594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|name|category|\n",
      "+----+--------+\n",
      "| dog|     pet|\n",
      "| cat|     pet|\n",
      "|bear|    wild|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying the Deduplicated DataFrame\n",
    "deduplicated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d6db5",
   "metadata": {},
   "source": [
    "You can see that duplicate entries have been removed, and you're left with a more organized and accurate dataset. This whole process shows how Spark helps with managing and cleaning up data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fae7e2",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa593ae",
   "metadata": {},
   "source": [
    "An RDD, or Resilient Distributed Dataset, is a fundamental data structure in Apache Spark, a popular open-source framework for distributed data processing. RDDs serve as the building blocks of Spark computations and provide a way to work with data in a distributed and fault-tolerant manner. Let's break down what RDDs are, how they work, why we use them, and some use cases.\n",
    "\n",
    "### What is an RDD?\n",
    "\n",
    "An RDD is a distributed collection of data that can be processed in parallel across a cluster of computers. RDDs are designed to handle large-scale data processing tasks by allowing the data to be partitioned across multiple nodes in a cluster. These partitions are processed in parallel, enabling efficient and scalable data processing.\n",
    "\n",
    "### How does an RDD work?\n",
    "\n",
    "+ **Creation:** RDDs can be created from data stored in distributed storage systems (like HDFS), from data in memory, or by transforming existing RDDs through operations like map, filter, and reduce.\n",
    "\n",
    "+ **Transformation:** RDDs are immutable, which means you can't modify them directly. Instead, you transform an RDD into a new RDD through transformations like mapping (applying a function to each element), filtering, joining, etc. These transformations are performed lazily, meaning the actual computation is deferred until an action is called.\n",
    "\n",
    "+ **Action:** Actions are operations that return non-RDD values, like counting elements, collecting data, or saving it to storage. When an action is invoked, the sequence of transformations leading to that action is computed.\n",
    "\n",
    "+ **Fault Tolerance:** RDDs are fault-tolerant. If a partition of an RDD is lost due to node failure, Spark can automatically reconstruct the lost partition using lineage information (a record of transformations).\n",
    "\n",
    "### Why do we use RDDs?\n",
    "\n",
    "+ **Ease of Use:** RDDs provide a high-level abstraction for distributed data processing, making it easier to write parallel and fault-tolerant code.\n",
    "\n",
    "+ **Scalability:** RDDs allow data to be divided into partitions and processed in parallel, which scales well across a cluster of machines.\n",
    "\n",
    "+ **Fault Tolerance:** RDDs automatically recover from node failures by recomputing lost data using lineage information.\n",
    "\n",
    "+ **Versatility:** RDDs can store any type of data, including structured and unstructured data. They're not limited to tabular data.\n",
    "\n",
    "### Use Cases of RDDs:\n",
    "\n",
    "+ **Big Data Processing:** RDDs are used for processing large-scale datasets efficiently and in parallel. This is crucial in scenarios where traditional single-node processing is not feasible.\n",
    "\n",
    "+ **Data Transformation:** RDDs enable various data transformations, like filtering out irrelevant data, transforming data into a new format, or aggregating data.\n",
    "\n",
    "+ **Iterative Algorithms:** Many machine learning algorithms involve iterative computations (like gradient descent). RDDs can efficiently support such algorithms by reusing data in memory.\n",
    "\n",
    "+ **Log Analysis:** Analyzing logs from various sources (web servers, applications, etc.) involves handling large amounts of unstructured data. RDDs can be used to process, filter, and aggregate log data.\n",
    "\n",
    "+ **Real-time Processing:** RDDs are used in stream processing scenarios, allowing for real-time data analysis by processing data in micro-batches.\n",
    "\n",
    "In summary, RDDs are a fundamental concept in Spark, providing a distributed and fault-tolerant way to process and analyze data efficiently across a cluster of machines. They're versatile and widely used in big data processing, machine learning, and other data-intensive applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7fc46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
