{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1237c1",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17f6c7",
   "metadata": {},
   "source": [
    "If you've spent time working with SQL, which stands for Structured Query Language, you probably know about JOINs. If not, here's a quick explanation: In SQL, JOIN is used to combine two or more tables of data by using a shared or related column(s).\n",
    "\n",
    "JOINing is useful for various tasks, such as adding more information from a smaller dataset to a larger one, or searching for data in reference datasets.\n",
    "\n",
    "There are different types of joins, like left, right, inner, and full outer. In Spark, especially with DataFrames, there are multiple ways to perform these joins. You can do all of this using the join() method.\n",
    "\n",
    "Just to clarify, in our discussion, we might use the words \"table\" and \"dataframe\" interchangeably. This is because, for our purposes here, they essentially mean the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643bbe3",
   "metadata": {},
   "source": [
    "!['Sql Joins'](sqljoin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1a90c",
   "metadata": {},
   "source": [
    "- **Inner Join:** This type of join returns records that have matching values in both tables being joined. Rows that don't have matches on both sides are excluded.\n",
    "\n",
    "- **Left (Outer) Join:** With a left join, you get all records from the left table and only the matching records from the right table. It's useful when you want all the rows from the left table along with matching rows from the right table. It's like adding information from the right table to the left table.\n",
    "\n",
    "- **Right (Outer) Join:** The right join is similar to the left join, but it returns all records from the right table and only the matching records from the left table. It's handy when you need all the rows from the right table along with matching rows from the left table. Think of it as flipping the perspective of the left join.\n",
    "\n",
    "- **Full (Outer) Join:** This type of join returns all records from both the left and right tables, regardless of whether they have matching values or not. It's useful when you want to combine rows from both tables while keeping non-matching rows from each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f0124a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\sparkhome'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c104adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c592dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Vamsi_App_2').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa1277",
   "metadata": {},
   "source": [
    "we have two lists of tuples: one containing animals and their categories, and the other with animals and their foods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26cd3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_animals = [(\"dog\", \"pet\"), (\"cat\", \"pet\"), (\"bear\", \"wild\")]\n",
    "animal_foods = [(\"dog\", \"kibble\"), (\"cat\", \"canned tuna\"), (\"bear\", \"salmon\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53713c2",
   "metadata": {},
   "source": [
    "We turn these lists into RDDs and then create DataFrames from the RDDs using Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308d89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "animalDataRDD = spark.sparkContext.parallelize(categorized_animals)\n",
    "animalFoodRDD = spark.sparkContext.parallelize(animal_foods)\n",
    "\n",
    "animalData = spark.createDataFrame(animalDataRDD, ['name', 'category'])\n",
    "animalFoods = spark.createDataFrame(animalFoodRDD, ['animal', 'food'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0961693",
   "metadata": {},
   "source": [
    "Next, we join the two DataFrames based on the common column, which is the animal name, and print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4cf115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+-----------+\n",
      "|name|category|animal|       food|\n",
      "+----+--------+------+-----------+\n",
      "|bear|    wild|  bear|     salmon|\n",
      "| cat|     pet|   cat|canned tuna|\n",
      "| dog|     pet|   dog|     kibble|\n",
      "+----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animals_enhanced = animalData.join(animalFoods, animalData.name == animalFoods.animal)\n",
    "animals_enhanced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0d507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "animals_enhanced_1 = animalFoods.join(animalData, animalFoods.animal == animalData.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd85a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+--------+\n",
      "|animal|       food|name|category|\n",
      "+------+-----------+----+--------+\n",
      "|  bear|     salmon|bear|    wild|\n",
      "|   cat|canned tuna| cat|     pet|\n",
      "|   dog|     kibble| dog|     pet|\n",
      "+------+-----------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animals_enhanced_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8375c",
   "metadata": {},
   "source": [
    "By doing this, we've successfully combined the data about animals, their categories, and their foods, creating a unified dataset.\n",
    "\n",
    "Remember, this is just one way to perform data joins in Spark. There's another approach called the \"usingColumn\" approach within the .join() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9b606",
   "metadata": {},
   "source": [
    "### The usingColumn Join Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff9bf2",
   "metadata": {},
   "source": [
    "Using the \"usingColumn\" approach in a join is useful when the left and right data sets have the same column name for the join key. This method provides a simpler and more straightforward way to achieve the same results as we did in the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b835b1b",
   "metadata": {},
   "source": [
    "We can try the same thing in PYTHON using the following code:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67c54a36",
   "metadata": {},
   "source": [
    "leftData.join(rightData, on=\"columnInCommon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2942e035",
   "metadata": {},
   "source": [
    "There's an advanced version of this approach that lets you include multiple columns in the join. You just need to provide a sequence of the relevant columns you want to join on in Scala or a list in Python. If you're acquainted with the JOIN USING clause in SQL, this is quite similar. Remember, every value in the list must be present on both sides of the join for this method to work successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bab207",
   "metadata": {},
   "source": [
    "We can achieve the same in Python using the following code:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c60babe3",
   "metadata": {},
   "source": [
    "leftData.join(rightData, on=['firstColumnInCommon', 'secondColumnInCommon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac9ddea",
   "metadata": {},
   "source": [
    "These techniques are not the sole methods for joining data, but they are widely used and clear-cut. This explanation also doesn't delve into the inherent computational outcomes of data joins, but we can explore that in a separate exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6be11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
