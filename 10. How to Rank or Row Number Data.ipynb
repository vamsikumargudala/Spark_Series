{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9f5d3f",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5abf4",
   "metadata": {},
   "source": [
    "Ranking in Spark, at its core, is a way to order data based on a certain condition. It combines the functionalities of both the WHERE clause (to filter data) and the ORDER BY clause (to sort data), but it doesn't remove data; instead, it assigns a numerical label to each row based on the specified condition or column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abd920",
   "metadata": {},
   "source": [
    "While ordering helps you sort data based on a specific column, ranking lets you assign a number or rank to each row. This rank can then be used in various ways downstream, such as selecting the top results or applying further transformations based on these assigned labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd900d85",
   "metadata": {},
   "source": [
    "A common ranking function in Spark is row_number(). This function allows you to assign a unique rank or value to each row, either within the entire dataset or within specific partitions of the data, based on a defined ordering. This result can be extremely useful for tasks like performing \"top n\" analysis, where you want to identify and work with the top results in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079fdd1",
   "metadata": {},
   "source": [
    "Ranking provides a powerful way to organize and manipulate data, especially when you need to prioritize or filter data based on specific criteria. It's a valuable tool in data analysis and processing tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6565e1",
   "metadata": {},
   "source": [
    "#### In this Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fea5f",
   "metadata": {},
   "source": [
    "we aim to find the top 2 cats and dogs in each category using ranking. While it's possible to answer this question without ranking, ranking simplifies the process and makes the results more readable and understandable. Let's see how we can achieve this."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e8f9296",
   "metadata": {},
   "source": [
    "We'll use code examples that may include lines like this:\n",
    "\n",
    "df.withColumn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fecfb",
   "metadata": {},
   "source": [
    "This code allows us to add a new column or modify an existing one in a DataFrame. It's commonly used to apply logic to a specific column in a DataFrame or DataSet. There's also a variant called **df.withColumnRenamed()** that both applies logic and renames the column simultaneously.\n",
    "\n",
    "To find the top 2 cats and dogs in each category using ranking, we'll use Spark's ranking functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "078c76fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\sparkhome'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing lib and functions\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5561a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c49a1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Vamsi_App_6').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe05fb",
   "metadata": {},
   "source": [
    "It looks like you've provided a detailed set of code and steps for working with Spark to find the top 2 cats and dogs in each category using ranking. The provided code demonstrates how to import libraries, create an RDD and DataFrame, set up a temporary table view, create a window, apply the **row_number()** function, and filter the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8cb48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant Spark libraries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c8a21",
   "metadata": {},
   "source": [
    "These allow us to access two key components in our code: **the windowing specification and the row_number ranking function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4b1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of Rows, each containing a name, type, age, and color\n",
    "my_previous_pets = [\n",
    "    (\"fido\", \"dog\", 4, \"brown\"),\n",
    "    (\"annabelle\", \"cat\", 15, \"white\"),\n",
    "    (\"fred\", \"bear\", 29, \"brown\"),\n",
    "    (\"daisy\", \"cat\", 8, \"black\"),\n",
    "    (\"jerry\", \"cat\", 1, \"white\"),\n",
    "    (\"fred\", \"parrot\", 1, \"brown\"),\n",
    "    (\"gus\", \"fish\", 1, \"gold\"),\n",
    "    (\"gus\", \"dog\", 11, \"black\"),\n",
    "    (\"daisy\", \"iguana\", 2, \"green\"),\n",
    "    (\"rufus\", \"dog\", 10, \"gold\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa24836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use parallelize() to create an RDD from the list\n",
    "petsRDD = spark.sparkContext.parallelize(my_previous_pets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53541683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the RDD with a provided schema\n",
    "petsDF = spark.createDataFrame(petsRDD,['nickname', 'type', 'age', 'color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4427277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary table view of the data in Spark SQL\n",
    "petsDF.createOrReplaceTempView('pets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef08f9d",
   "metadata": {},
   "source": [
    "#### Create a Window that is partitioned by 'type' and orders by 'age' in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb129a34",
   "metadata": {},
   "source": [
    "Let's break down the components of this operation:\n",
    "\n",
    "+ **Window:** In Spark, a window is a logical construct used for defining a set of rows over which a function is applied. It specifies the range or scope of rows that should be considered when applying functions like **row_number(), rank(), or aggregation functions**.\n",
    "\n",
    "+ **Partitioning:** The term \"partitioned by 'type'\" means that the data is divided into groups or partitions based on the values in the 'type' column. Each partition is treated as a separate group for window functions. For example, if you have 'cat' and 'dog' as values in the 'type' column, the window function will operate separately within these two groups.\n",
    "\n",
    "+ **Ordering:** The window is ordered by the 'age' column, and it specifies \"in descending order.\" This means that within each partition (group), the rows are sorted by the 'age' column in descending order. In other words, the rows with the highest 'age' values come first within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f456a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"type\").orderBy(F.col(\"age\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bec7beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.window.WindowSpec object at 0x000002417E16E790>\n"
     ]
    }
   ],
   "source": [
    "print(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1bcfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use withColumn() and row_number() to apply the Windowing function and create a 'rank' column\n",
    "# Filter down to the top two ranks of each group: cats and dogs\n",
    "# Print the results\n",
    "resultDF = petsDF.withColumn(\"rank\", F.row_number().over(window))\\\n",
    "    .where(\"rank <= 2 and (type = 'dog' or type = 'cat')\")\\\n",
    "    .orderBy(\"type\", \"rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee02499",
   "metadata": {},
   "source": [
    "Defines a window, applies the row_number() function, and filters the top 2 ranks for cats and dogs. Finally, it prints the results as shown in your provided output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbb7336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---+-----+----+\n",
      "| nickname|type|age|color|rank|\n",
      "+---------+----+---+-----+----+\n",
      "|annabelle| cat| 15|white|   1|\n",
      "|    daisy| cat|  8|black|   2|\n",
      "|      gus| dog| 11|black|   1|\n",
      "|    rufus| dog| 10| gold|   2|\n",
      "+---------+----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37420119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
