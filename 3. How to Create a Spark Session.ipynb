{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81a9568",
   "metadata": {},
   "source": [
    "### How to Create a Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db211a5",
   "metadata": {},
   "source": [
    "![\"Spark Image\"](spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560a835",
   "metadata": {},
   "source": [
    "Image courtesy of DataBricks: [How to use Spark Session in Apache Spark 2.0](https://www.databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee51cf",
   "metadata": {},
   "source": [
    "The basic building block for working with Spark in a modern context. In shorter terms: it’s the modern entry point into Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407ed25",
   "metadata": {},
   "source": [
    "The Spark Session was [introduced in the 2.0 release of Apache Spark](https://www.databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html), and was designed to both replace and consolidate the previous methods for accessing Spark: contexts! Sessions also made it easier to:\n",
    "\n",
    "+ configure runtime properties of Spark applications after instantiation, (e.g. set spark.sql.shuffle.partitions)\n",
    "+ create DataFrames and DataSets\n",
    "+ use Spark SQL and access Hive (or Glue, or equivalent)\n",
    "\n",
    "So, instead of using the Spark Context, SQL Context and Hive Context objects of the past, you can simply use a Spark Session. You maintain all of the convenience of those legacy objects with one straightforward instantiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73746aa9",
   "metadata": {},
   "source": [
    "***The steps for creating a Spark Session are provided in Python languages: Select your API of choice and proceed!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a8d366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\sparkhome'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39ad2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df70eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"My Spark App\")\\\n",
    "    .master(\"local[2]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c12db7",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "***.master()*** is not always required, but allows you to establish where your Spark application should run. If you choose local[*], where * is the number of worker threads to use, you’ll be running a local Spark application on your local machine. If you choose YARN, you’ll leverage an existing Hadoop cluster upon which a running Spark application should recognize. In notebook environments like Databricks or Qubole Notebooks, it’s unlikely you need to create a session at all — just use the provided spark (i.e. session) variable for your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e273efe",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "Now, you’re ready to start using Spark. That might not have been very exciting, but in the next exercise we’ll dive headfirst into our first actual Spark application — data deduplication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695765e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
