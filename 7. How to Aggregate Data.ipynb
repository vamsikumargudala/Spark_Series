{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65125117",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46b9d8",
   "metadata": {},
   "source": [
    "**Aggregation**, also referred to as **grouping**, is a technique used to condense data by categorizing it into understandable groups. Simultaneously, within these groups, you can also analyze data points using SQL-like aggregate functions (functions that you can use while summarizing data).\n",
    "\n",
    "Imagine you operate an online shoe store. You possess data records that provide details about each customer's purchases and the corresponding dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885f1530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer    shoe_name  sale_price purchase_date\n",
      "0   landon   blue shoes         5.0    2020-10-01\n",
      "1    james   blue shoes         5.0    2020-10-04\n",
      "2     zach  white shoes         6.0    2020-10-06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"customer\": [\"landon\", \"james\", \"zach\"],\n",
    "    \"shoe_name\": [\"blue shoes\", \"blue shoes\", \"white shoes\"],\n",
    "    \"sale_price\": [5.00, 5.00, 6.00],\n",
    "    \"purchase_date\": [\"2020-10-01\", \"2020-10-04\", \"2020-10-06\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d47495",
   "metadata": {},
   "source": [
    "At the end of each month, you might want to summarize the data to see how much revenue each shoe model generated during that month. In SQL, you could achieve this with a query like:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c9a6f91",
   "metadata": {},
   "source": [
    "SELECT SUM(sale_price) AS revenue, \n",
    "       shoe_name \n",
    "FROM sales \n",
    "GROUP BY shoe_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c905f",
   "metadata": {},
   "source": [
    "In this example, \"shoe_name\" is the field we're using to group the data, and the sum of sales for each shoe model is the aggregation measure. The expected outcome would be:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4d8a612",
   "metadata": {},
   "source": [
    "shoe_name     revenue\n",
    "blue shoes    10.00\n",
    "white shoes   6.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e2f83",
   "metadata": {},
   "source": [
    "This result indicates that there were two sales of blue shoes at $5 per pair and one sale of white shoes at $6 per pair. If we only used SUM(sale_price) AS revenue without the GROUP BY shoe_name, we'd only get the total revenue of $16. It wouldn't tell us which shoes were more popular.\n",
    "\n",
    "\"SUM()\" is just one type of aggregate function. There are others like \"MIN()\", \"MAX()\", \"COUNT()\", and \"STDEV()\", among others. These functions help summarize distinct groups within your data.\n",
    "\n",
    "In the upcoming exercises, we'll explore how to analyze data in groups using aggregation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe9936",
   "metadata": {},
   "source": [
    "#### Exercise Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf6ca5",
   "metadata": {},
   "source": [
    "Import relevant Spark SQL libraries using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d69679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\sparkhome'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03368caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f956f",
   "metadata": {},
   "source": [
    "Continuing to build on our animal data set (from previous entries in this course), Create a List of Rows, each containing an animal name, type, age and color using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e65b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of Rows containing pet data\n",
    "my_previous_pets = [(\"fido\", \"dog\", 4, \"brown\"),\n",
    "                    (\"annabelle\", \"cat\", 15, \"white\"),\n",
    "                    (\"fred\", \"bear\", 29, \"brown\"),\n",
    "                    (\"gus\", \"parakeet\", 2, \"black\"),\n",
    "                    (\"daisy\", \"cat\", 8, \"black\"),\n",
    "                    (\"jerry\", \"cat\", 1, \"white\"),\n",
    "                    (\"fred\", \"parrot\", 1, \"brown\"),\n",
    "                    (\"gus\", \"fish\", 1, \"gold\"),\n",
    "                    (\"gus\", \"dog\", 11, \"black\"),\n",
    "                    (\"daisy\", \"iguana\", 2, \"green\"),\n",
    "                    (\"rufus\", \"dog\", 10, \"gold\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24143fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fido', 'dog', 4, 'brown'),\n",
       " ('annabelle', 'cat', 15, 'white'),\n",
       " ('fred', 'bear', 29, 'brown'),\n",
       " ('gus', 'parakeet', 2, 'black'),\n",
       " ('daisy', 'cat', 8, 'black'),\n",
       " ('jerry', 'cat', 1, 'white'),\n",
       " ('fred', 'parrot', 1, 'brown'),\n",
       " ('gus', 'fish', 1, 'gold'),\n",
       " ('gus', 'dog', 11, 'black'),\n",
       " ('daisy', 'iguana', 2, 'green'),\n",
       " ('rufus', 'dog', 10, 'gold')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_previous_pets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da4773",
   "metadata": {},
   "source": [
    "Use the parallelize() function of Spark to turn that List into an RDD, create a DataFrame from it by providing a schema, and make a temp SQL view on top of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e60d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b96544",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Vamsi_App_3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d0ede3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "petsRDD = spark.sparkContext.parallelize(my_previous_pets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8006a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the RDD with a specified schema\n",
    "petsDF = spark.createDataFrame(petsRDD, ['nickname', 'type', 'age', 'color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3f3d5",
   "metadata": {},
   "source": [
    "### Create a SQL view using the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd2fb4",
   "metadata": {},
   "source": [
    "The **createOrReplaceTempView** function in Spark creates a temporary view (or table) that allows you to interact with a DataFrame using SQL queries. Here's what it means in more detail:\n",
    "\n",
    "+ **Create:** It creates a temporary view based on the DataFrame you provide.\n",
    "\n",
    "+ **Or Replace:** If a temporary view with the same name already exists, this function will replace it with the new one you're creating.\n",
    "\n",
    "+ **Temp View:** The view you create is temporary, meaning it only exists within the context of your current Spark session and is not persisted to disk. It's meant for quick querying and analysis during your current session.\n",
    "\n",
    "In your example, after creating the DataFrame petsDF, you use the createOrReplaceTempView function to make a temporary view named 'pets'. This view allows you to run SQL queries on the DataFrame as if it were a SQL table. This can be quite useful when you want to perform analysis using SQL syntax rather than DataFrame methods. Just remember that this view is only available for the duration of your Spark session and won't be accessible after you close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12e2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQL view using the DataFrame\n",
    "petsDF.createOrReplaceTempView('pets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b33f56",
   "metadata": {},
   "source": [
    "### Analysis through Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db7a06",
   "metadata": {},
   "source": [
    "We now have an in-memory view, similar to a relational table, of our data. This view only exists within the scope of our Spark application, but it allows us to execute SQL queries against it as if it were an actual table in a database. This feature proves to be very useful in your work with Spark.\n",
    "\n",
    "With our queryable view in place, let's address various questions about the data using Spark's powerful SQL capabilities to query the temporary view we've set up. Once a table is registered, you can query it multiple times, treating it just like a real table.\n",
    "\n",
    "Now, let's look at the Python solutions for the given questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf1332",
   "metadata": {},
   "source": [
    "**Q. What are the three most popular (i.e. recurring) names in the data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130a27b",
   "metadata": {},
   "source": [
    "To answer this question, we need to write a SQL query that counts the occurrences of each name in the table. While you could achieve this using DataFrame methods, we'll focus on the SQL approach in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a70f6792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---+-----+\n",
      "| nickname|    type|age|color|\n",
      "+---------+--------+---+-----+\n",
      "|     fido|     dog|  4|brown|\n",
      "|annabelle|     cat| 15|white|\n",
      "|     fred|    bear| 29|brown|\n",
      "|      gus|parakeet|  2|black|\n",
      "|    daisy|     cat|  8|black|\n",
      "|    jerry|     cat|  1|white|\n",
      "|     fred|  parrot|  1|brown|\n",
      "|      gus|    fish|  1| gold|\n",
      "|      gus|     dog| 11|black|\n",
      "|    daisy|  iguana|  2|green|\n",
      "|    rufus|     dog| 10| gold|\n",
      "+---------+--------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pets\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "659f6c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|nickname|occurrences|\n",
      "+--------+-----------+\n",
      "|     gus|          3|\n",
      "|    fred|          2|\n",
      "|   daisy|          2|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The following shows the code in SCALA and PYTHON:\n",
    "spark.sql(\"select nickname, count(*) as occurrences from pets group by nickname order by occurrences desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e965417",
   "metadata": {},
   "source": [
    "As can be seen the three most popular names are gus, fred, and daisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d6df3",
   "metadata": {},
   "source": [
    "**Q. How old is the oldest cat in the data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c291c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "| nickname|age|\n",
      "+---------+---+\n",
      "|annabelle| 15|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT nickname, max(age) as age FROM pets WHERE type='cat' GROUP BY nickname limit 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81812f30",
   "metadata": {},
   "source": [
    "We can determine the maximum age of cats using Spark's functional API. Although you could achieve this with pure SQL as well, this example will focus on the functional approach.\n",
    "\n",
    "The output will provide the age of the oldest cat in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417a75a",
   "metadata": {},
   "source": [
    "+ Use the where() function of the DataFrame to filter the data to just cats.\n",
    "+ Use the agg() function of the DataFrame to select the max age.\n",
    "+ Use the show() function of the DataFrame to print the results to the console.\n",
    "\n",
    "In code form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e731046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      15|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "petsDF.where(\"type = 'cat'\")\\\n",
    ".agg({\"age\": \"max\"})\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c8836",
   "metadata": {},
   "source": [
    "**Q. What are the youngest and oldest cat ages?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701bd2f",
   "metadata": {},
   "source": [
    "+ Use the where() function of the DataFrame to filter the data to just cats.\n",
    "+ Group the data by type using groupBy().\n",
    "+ Then, combine the agg() function of the DataFrame with the min() and max() functions to request two metrics: min and max age. Optional: rename the columns using alias().\n",
    "        \n",
    "Finally, print the results to the console using the show() function of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4455187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+\n",
      "|type|min(age)|max(age)|\n",
      "+----+--------+--------+\n",
      "+----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "petsDF.where(petsDF[\"type\"] == \"cats\").groupby(\"type\").agg(F.min(\"age\"),F.max(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eaccfce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+\n",
      "|type|min(age)|max(age)|\n",
      "+----+--------+--------+\n",
      "| cat|       1|      15|\n",
      "+----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "petsDF.where(petsDF[\"type\"] == \"cat\") \\\n",
    ".groupBy(\"type\") \\\n",
    ".agg(F.min(\"age\"), F.max(\"age\")) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9beb851",
   "metadata": {},
   "source": [
    "**Q. What is the average dog age?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd0d56f",
   "metadata": {},
   "source": [
    "+ Use the where() function of the DataFrame to filter the data to just dogs.\n",
    "+ Group the data by type using groupBy().\n",
    "+ Use the agg() function of the DataFrame to select the average age.\n",
    "\n",
    "Finally, print the results to the console using the show() function of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84c4e862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         avg(age)|\n",
      "+-----------------+\n",
      "|8.333333333333334|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "petsDF.where(\"type = 'dog'\")\\\n",
    ".agg({\"age\": \"avg\"})\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef91fbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|type|         avg(age)|\n",
      "+----+-----------------+\n",
      "| dog|8.333333333333334|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "petsDF.where(\"type = 'dog'\")\\\n",
    ".groupBy(\"type\")\\\n",
    ".agg(F.avg(\"age\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c314c36",
   "metadata": {},
   "source": [
    "**Q. How many pets of each color are there in the data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673ac32",
   "metadata": {},
   "source": [
    "+ Group the data by type using groupBy().\n",
    "+ Use the groupBy() function of the DataFrame to count the records in each group.\n",
    "\n",
    "Finally, print the results to the console using the show() function of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc957848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|color|count|\n",
      "+-----+-----+\n",
      "|brown|    3|\n",
      "|white|    2|\n",
      "|black|    3|\n",
      "| gold|    2|\n",
      "|green|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "petsDF.groupBy(\"color\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea1084",
   "metadata": {},
   "source": [
    "The number of pets in each color category can be seen here.\n",
    "\n",
    "And that’s a rough introduction to aggregation in Spark / Spark SQL! There are a lot of powerful operations you can conduct in Spark, so keep exploring the APIs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716d2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
